\section{Related Work}
\header{SAG.}
\tool extends and adapts \emph{SAG} into matrix factorization for data-scientists prototyping recommender systems.
\emph{SAG} \cite{schmidt2013minimizing, roux2012stochastic} scales convex-optimzation in supervised machine-learning to a large number of samples, and a large number of features.
Block-parallel \emph{SAG} ~\cite{bpsag} is another extension of \emph{SAG}.
It parallelizes \emph{SAG} to a high number of threads and aims to make the marginal gain of an additional thread linear rather than submodular.


\header{Speeding up Matrix Factorization.}
Prior work on speeding up matrix factorization focus on extending gradient methods that are already being in use.
For example \cite{gemulla2011large} distributes stochastic gradient;
\cite{yu2012scalable} parallelizes coordinate descent.
Both focus on scalability while \tool focuses on prototyping.

\tool advocates a new gradient method that has never been used in matrix factorization.  
\tool is implemented in Matlab and takes advantage of Matlab's parallel computing toolkit for distribution and parallelization.
\tool is architected so that ideally data-scientists do not have to learn any new material for using \tool


\header{Alternating Least Squares.}
Alternating least squares \emph{ALS} \cite{koren2009matrix, takacs2012alternating} speeds up full deterministic gradient (\emph{FG}) in matrix factorization.
At any point in time, \emph{ALS} fixes one matrix (e.g. $U$) of the two matrices, and optimizes only the other (e.g. $V$).
After optimizing a matrix (e.g. $V$) to a certain point, \emph{ALS} switches order: fix $V$ and optimize $U$.

\tool works with a broader class of functions because \emph{ALS} works only with least-squares.
Adopting the alternating approach in \emph{SAG} would not asymptotically save space:
we still have to store the different fine-grain gradients that we calculated previoulsy at different iterations in the past.

Combining the chain rule with alternating \emph{SAG} would both save space and avoid re-computation.
When we fix one of the two matrices, the problem becomes highly similar to using the chain rule in supervised machine-learning.  

As discussed in the \emph{challenge} section, using the chain rule is undesirable.
Many objective functions in recommender systems learn how to rank items for an user.
Learning to rank often requires comparing the \emph{relative} difference between all approximiated entries that belong to the same row:
e.g. $\left(\bar{u}_{i}*\bar{v}_{j} - \bar{u}_{i}*\bar{v}_{k}\right)$ for all $k \neq j$.
Chain rule cannot be used for these functions \cite{gapfm, climf, bpr, mnar, mmmf}, because 
the chain rule works with only functions that uses only the \emph{absolute} value of an approximiated entry e.g. ($\hat{a}_{ij} = \bar{u}_{i}*\bar{v}_{j}$).
