\subsection{Algorithm}

\subsection{Convergence, Time and Space}
Theorem 1: SAG-re has convergence rate at least as fast as the original SAG.

Theorem 2: Despite re-computation, SAG-re has asymptotic time-complexity as efficient as any gradient method having the lowest iteration cost, namely stochastic gradient.

Theorem 3: Despite storing the summed-up memory gradients, SAG-re has asymptotic space-complexity as compact as memory-less gradient methods.
