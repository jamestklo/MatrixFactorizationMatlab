\section{Challenge}
As equations \ref{eq:sag_mu} and \ref{eq:sag_mv} illustrate, updating $\bar{m}_{U}^{t+1}$ and $\bar{m}_{V}^{t+1}$ requires $\bar{m}_{entry(b).i}^{t}$ and $\bar{m}_{entry(b).j}^{t}$.  
$\bar{m}_{entry(b).i}^{t}$ and $\bar{m}_{entry(b).j}^{t}$ are the fine-grained gradients of an individual entry $entry(b)$ from the last time (or the most recent time) that $entry(b)$ was sampled.  

When applying \emph{SAG} into matrix factorization, a major challenge is to make these fine-grain gradients available: 
$\bar{m}_{entry(b).i}^{t}$ from equation \ref{eq:sag_mu}, and 
$\bar{m}_{entry(b).j}^{t}$ from equation \ref{eq:sag_mv}  

A na$\ddot{i}$ve approach is to store all these fine-grain graidents.  
As we shall prove, the na$\ddot{i}$ve approach is undesirable because storing all these gradients would take up a lot of space.  


%\newtheorem{totalspace}{}  
\emph{Theorem 1.} 
The total asymptotic space complexity is $\theta(nDims*(min(M,N)+nRows+nCols))$ for the na$\ddot{i}$ve approach of storing the fine-grain gradients of all entries that we had previously sampled.  
\begin{proof}
For each individual entry, the amount of space required is $2*nDims$:  
the gradient with respect to row $\bar{u}_i$ ($\bar{m}_{entry(b).i}^{t}$) is a \emph{1}-by-\emph{nDims} row vector;  
the gradient with respect to column $\bar{v}_j$ ($\bar{m}_{entry(b).j}^{t}$) is a \emph{nDims}-by-\emph{1} column vector.  

When we store the fine-grain gradients of all previously-sampled entries, the amount of space required becomes $M*2*nDims$.  
Recalling from the background section, \emph{M} is the number of distinct entries that we previously sampled.  

As shown in equations \ref{eq:sag_mu} and \ref{eq:sag_mv}, \emph{SAG} requires only the most recent gradient of each previously-sampled entry.
Thus for each entry, we store a max of only one set of gradients ($\bar{m}_{entry(b).i}^{t}$ and $\bar{m}_{entry(b).j}^{t}$).
The total amount of space required becomes $min(M,N)*2*nDims$.

Now, according to equations \ref{eq:sag_mu} and \ref{eq:sag_mv}, we must also store the aggregated gradients: $\bar{m}_{U}^{t}$ and $\bar{m}_{V}^{t}$.  
$\bar{m}_{U}^{t}$ takes $nRows*nDims$ space; $\bar{m}_{V}^{t}$ takes $nDims*nCols$ space.  
Thus the total amount of space that we use to store the aggregated gradients is ($nRows*nDims$) + ($nDims*nCols$), which is equivalent to $nDims*(nRows+nCols)$ after simplification. 

Adding the fine-grain gradients and the aggregated gradients together, the asymptotic space complexity becomes $\theta(nDims*(min(M,N)+nRows+nCols))$ after ignoring the constants.  
\end{proof}


\header{No guarantee that $min(M,N)$ is small.}
If we can guarantee that $min(M,N)$ is small, or that $min(M,N)$ is asymptotically not larger than $nRows$ or $nCols$, 
then the effective asymptotic space-complexity becomes $\theta(nDims*(nRows+nCols))$, which is the most compact anyone can possibly get.  Unfortunatley, we shall prove that there is no such guarantee.  

First, we explore what the best possible asymptotical space-complexity can be in matrix factorization.


\emph{Theorem 2.}
$\Omega(N+nDims*(nRows+nCols))$ is the lower-bound asymptotic space-complexity in matrix factorization.
\begin{proof}  
Matrix factorization is to approximate a matrix $A$ (e.g. the \emph{user-item} matrix) through the dot product of two matrices $U$ (e.g. the \emph{user} matrix) and $V$ (e.g. the \emph{item} matrix). 
$A$ has $N$ non-zero entries.  $U$ is a $nRows$-by$nDims$ matrix; $V$ is a $nDims$-by-$nCols$ matrix.  
In each iteration of convex optimization, we must update $U$ and $V$, and use an objective function to compare our approximation to the ground-truth matrix $A$.  
Therefore, any matrix factorization algorithm would have an asymptotic space-complexity of at least $\Omega(N+nDims*(nRows+nCols))$.
\end{proof}


If we can guarantee that $min(M,N)$ is asymptotically not larger than $nRows$ or $nCols$, 
then we can prove that the na$\ddot{i}$ve approach has already achieved the best possible asymptotic space-complexity, and that our challenge is irrelevant.
However, we shall prove that such guarantee does not exist.


\emph{Theorem 3.}  
There is no guarantee that $min(M,N)$ is asymptotically not larger than $nRows$ or $nCols$.
\begin{proof}
$N$ is the number of non-zero entries in the matrix $A$.
Unless there is, or unless we are restricted to an upper-bound of matrix density, then $N$ must have an uppder-bound of $O(nRows*nCols)$ space.

$M$ is the number of \emph{distinct} entries that we previously sampled.
According to equation \ref{eq:msampled}, $M$ depends on the batch size at each iteration $B_l$, and the number of iterations previously done $t-1$.
Usually, the batch size is a constant $B$.  Thus the lower bound of $M$ most likely depends on the lower bound of $t$.
However, the lower bound of $t$ depends on the convergence rate, and the tolerance of error $\epsilon$.
For example, if the convergence rate is exponential (e.g. $O(p^t)$), then the lower bound of $t$ is $\Omega(log(\frac{1}{\epsilon}))$.
%$p$ is a constant calculated from the Hessians of the objective function; $0 \leq p \leq 1$.
%Convergence rate depends on the combination of both the underlying objective function, and the gradient method being used.
%For example, for a given objective function, both full deterministic gradient and stochastic average gradient may yield the fast exponential convergence $O(p^t)$.
Therefore, the lower bound of $M$ does not depend on $N$, $nRows$ or $nCols$.
Given a dataset, the only way to enforce $M \leq N$ is to either tolerate a high error, or to find a combination of objective function and gradient method that yields the fastest convergence rate  possible.
The asymptotic space-complexity of \tool is compact enough so that \tool does not enforce data-scientists to tolerate a high error.
Given any objective function, the convergence rate of \emph{SAG} \cite{schmidt2013minimizing, roux2012stochastic} is always faster than stochastic gradient and is sometimes as fast as the fastest full deterministic gradient.
\tool preserves the convergence rate of \emph{SAG}.
\end{proof}


\header{Using chain rule worsens space-complexity in matrix factorization.}
In supervised machine-learning, we can use the chain-rule in differential-calculus to reduce space-complexity.
Unfortunately, applying the chain-rule in matrix-factorization would result in a space-complexity larger than the na$\ddot{i}$ve approach.

In supervised machine-learing, the goal is to compute the best-fit column-vector $\bar{\omega}$ that optimizes an objective function, which can be written as
\begin{equation} \label{eq:sml}
\operatorname*{arg\,min (or\,arg\,max)}_{\bar{\omega}} \left[F(\hat{y} = X*\bar{\omega}) = \sum_{i=1}^{N} f_{i}(\hat{y}_{i} = \bar{x}_{i}*\bar{\omega}) \right]
\end{equation}

$X$ is a $N$-by-$d$ matrix: $N$ is the number of samples, and $d$ is the number of features.  
$\bar{x}_{i}$ is the $1$-by-$d$ row vector reprsenting $i$-th sample.
$\bar{\omega}$ is the $d$-by-$1$ column vector of features that we are trying to learn from $X$.
We can use the chain-rule and re-write the gradient of $\bar{\omega}$ with respect to $f_{i}$:
\begin{equation} \label{eq:chainsml}
\frac{\text{d}f_{i}}{\text{d}\bar{\omega}} = \left(\frac{\text{d}f_{i}}{\text{d}\hat{y}_{i}}\right)\frac{\text{d}\hat{y}_{i}}{\text{d}\bar{\omega}} = \left(\bar{x}_{i}\right)'\left(\frac{\text{d}f_{i}}{\text{d}\hat{y}_{i}}\right)
\end{equation}

Originally, using the na$\ddot{i}$ve approach of \emph{SAG} results in $\theta(min(M,N)*d+d)$ space.
The reason is that $\frac{\text{d}f_{i}}{\text{d}\bar{\omega}}$ is a $d$-by-$1$ column vector; and the na$\ddot{i}$ve approach stores $min(M,N)$ copies of them.  
The memory gradient $\bar{m}_{\bar{\omega}}$ is a $d$-by-$1$ column vector and thus takes $\theta(d)$ space.

The dot-product $\hat{y}_{i} = \left(\bar{x}_{i}*\bar{\omega}\right)$ is a $1$-by-$1$ scalar.  
Consequently, $\frac{\text{d}f_{i}}{\text{d}\hat{y}_{i}}$ is also a $1$-by-$1$ scalar.  
From equation \ref{eq:sml}, $\frac{\text{d}\hat{y}_{i}}{\text{d}\bar{\omega}}$ = $\left(\bar{x}_i\right)'$.
Therefore, we can apply the chain rule and reduce space-complexity to $\theta(min(M,N)+d)$, because we can use the vector $\bar{x}_{i}$ to re-compute $\frac{\text{d}f_{i}}{\text{d}\bar{\omega}}$ from the scalar $\frac{\text{d}f_{i}}{\text{d}\hat{y}_{i}}$.


\emph{Theorem 4.} 
Applying the chain-rule for using \emph{SAG} in matrix facotrization would result in $\theta(min(M,N) + nDims*(min(M,N)+nRows+nCols))$ space.
\begin{proof}
In matrix factorization, $\hat{a}_{ij} = \left(\bar{u}_{i}*\bar{v}_{j}\right)$ is a $1$-by-$1$ scalar.  
Therefore, we can rewrite the gradients as
\begin{equation}
\frac{\text{d}{f}}{\text{d}\bar{u}_{i}} = \left(\frac{\text{d}{f}}{\text{d}\hat{a}_{ij}}\right)\frac{\text{d}{\hat{a}_{ij}}}{\text{d}\bar{u}_{i}} = \left(\bar{v}_{j}\right)'\left(\frac{\text{d}{f}}{\text{d}\hat{a}_{ij}}\right)
\end{equation}
\begin{equation}
\frac{\text{d}{f}}{\text{d}\bar{v}_{j}} = \left(\frac{\text{d}{f}}{\text{d}\hat{a}_{ij}}\right)\frac{\text{d}{\hat{a}_{ij}}}{\text{d}\bar{v}_{j}} = \left(\bar{u}_{i}\right)'\left(\frac{\text{d}{f}}{\text{d}\hat{a}_{ij}}\right)
\end{equation}

$\left(\frac{\text{d}{f}}{\text{d}\hat{a}_{ij}}\right)$ is a $1$-by-$1$ scalar, and the chain-rule approach stores $min(M,N)$ copies, occupying $\theta(min(M,N))$ space.

Unfortunately both $U$ and $V$ change over time in matrix factorization.
When we apply the chain-rule, we cannot just use the current versions of $\bar{u}_{i}$ and $\bar{v}_{j}$.
We must use and thus must store the past versions of $\bar{u}_{i}^{l}$ and $\bar{v}_{j}^{l}$ at the last time $l$ that the entry $a_{ij}$ (in matrix $A$) was sampled.
Both $\bar{u}_{i}^{l}$ and $\bar{v}_{j}^{l}$ are vectors of length $nDims$.
Therefore, using the chain rule induces an additional ($min(M,N)*2*nDims$) space.
When we include the memory of aggregated gradients $\bar{m}_{U}$ and $\bar{m}_{V}$, the total space-complexity becomes larger than the na$\ddot{i}$ve approach with $\theta(min(M,N) + nDims*(min(M,N)+nRows+nCols))$ space.
The chain-rule approach yields space savings in supervised machine-learning because $\bar{x}_{i}$ does not change over time; 
so there is no need to store past versions of $\bar{x}_{i}$.
\end{proof}
