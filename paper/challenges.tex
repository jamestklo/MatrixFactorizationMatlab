\section{Challenge}
As equations \ref{eq:sag_mu} and \ref{eq:sag_mv} illustrate, updating $\bar{m}_{U}^{t+1}$ and $\bar{m}_{V}^{t+1}$ requires $\bar{m}_{entry(b).i}^{t}$ and $\bar{m}_{entry(b).j}^{t}$.  
$\bar{m}_{entry(b).i}^{t}$ and $\bar{m}_{entry(b).j}^{t}$ are the fine-grained gradients of an individual entry $entry(b)$ the last time (or the most recent time) that $entry(b)$ was sampled.  

When applying \emph{SAG} into matrix factorization, a major challenge is to make these fine-grain gradients available: 
$\bar{m}_{entry(b).i}^{t}$ from equation \ref{eq:sag_mu}, and 
$\bar{m}_{entry(b).j}^{t}$ from equation \ref{eq:sag_mv}  

A naive approach is to store these fine-grain graidents.  
As we shall proof, such naive approach is undesirable because storing all these gradients would take up a lot of space.  

\newtheorem{totalspace}{The total asymptotic space complexity of is }  
\begin{proof}[Proof]  
For each entry, the amount of memory required is $2*nDims$:  
the gradient with respect to row $\bar{u}_i$ ($\bar{m}_{entry(b).i}^{t}$) is a \emph{1}-by-\emph{nDims} row vector;  
the gradient with respect to column $\bar{v}_j$ ($\bar{m}_{entry(b).j}^{t}$) is a \emph{nDims}-by-\emph{1} column vector.  

We must store the fine-grain gradients of all entries that were sampled previously: $M*2*nDims$.  
Recalling the background section, \emph{M} is the number of distinct entries that we previously sampled.  
We store only one set of gradients ($\bar{m}_{entry(b).i}^{t}$ and $\bar{m}_{entry(b).j}^{t}$) for each entry, 
because \emph{SAG} requires only the most 
\end{proof}
