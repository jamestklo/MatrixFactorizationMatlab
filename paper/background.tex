\section{Background and Terminology}
To motivate our paper and the space complexity challenge, we first introduce the background and the terminology that we use.  


\header{Matrix Factorization.}
Model-based recommender systems approximate the \emph{user-item} matrix \emph{A} through 
the dot-product of the \emph{user}-matrix \emph{U} and the \emph{item}-matrix \emph{V}: $\hat{A} = U * V.$  

The \emph{user-item} matrix \emph{A} is a \emph{nRows}-by-\emph{nCols} matrix.
\emph{A} can be \emph{sparse}; thus we use \emph{N} to indicate the number of non-zero entries in \emph{A}.

The \emph{approximation} matrix $\hat{A}$ also has \emph{nRows} rows, and \emph{nCols} columns.  
$\hat{A}$ is not a sparse matrix.  The goal of model-based recommendation is to use the non-zero entries to approximate the missing entries in \emph{A}. 
When multiplying \emph{U} and \emph{V}, the latent dimensions \emph{nDims} cancels-out in the dot product. 
This is why the \emph{approximation} matrix has identical dimensions as the original \emph{user-item} matrix.  

The \emph{user} matrix \emph{U} is \emph{nRows}-by-\emph{nDims}: \emph{U} has \emph{nRows} rows, and \emph{nDims} columns.  
\emph{nDims} is the number of latent dimensions.  
The \emph{item} matrix \emph{V} is \emph{nDims}-by-\emph{nCols}.  


\header{Optimizing an Objective Function.}
The goal of matrix factorization is to find the best \emph{U} and the best \emph{V} whose dot product optimizes an objective function:
\begin{equation} \label{eq:argmin}
\operatorname*{arg\,min (or\,arg\,max)}_{U,V} \left[ f(U, V) = \sum_{i=1}^{nRows} \sum_{j=1}^{nCols} f(\bar{u}_{i}, \bar{v}_{j}) \right]
\end{equation}

When we take the gradient of the objective function with respect to a row in the \emph{user} matrix \emph{U} (e.g. $\bar{u}_{i}$), we sum up the gradient of all the entries in $\hat{A}$ that belong to the same row $\bar{u}_{i}$.  
\begin{equation} \label{eq:gradU}
\frac{\text{d}f(U,V)}{\text{d}\bar{u}_i} = \sum_{j=1}^{nCols} \frac{\text{d}f(\bar{u}_i,\bar{v}_j)}{\text{d}\bar{u}_i}
\end{equation}

Similarly, when we take the gradient with respect to a column of \emph{V} (e.g. $\bar{v}_{j}$), we sum up the gradients across different rows that belong to the same column: 
\begin{equation} \label{eq:gradV}
\frac{\text{d}f(U,V)}{\text{d}\bar{v}_j} = \sum_{i=1}^{nRows} \frac{\text{d}f(\bar{u}_i,\bar{v}_j)}{\text{d}\bar{v}_j}
\end{equation}

Both $\frac{\text{d}f(\bar{u}_i,\bar{v}_j)}{\text{d}\bar{u}_i}$ and $\frac{\text{d}f(\bar{u}_i,\bar{v}_j)}{\text{d}\bar{v}_j}$ are vectors of length \emph{nDims}, the number of latent dimensions.
Specifically, $\frac{\text{d}f(\bar{u}_i,\bar{v}_j)}{\text{d}\bar{u}_i}$ is a \emph{1}-by-\emph{nDims} row vector;
$\frac{\text{d}f(\bar{u}_i,\bar{v}_j)}{\text{d}\bar{v}_j}$ is a \emph{nDims}-by-\emph{1} column vector.
Similarly, the summed-up gradient $\frac{\text{d}f(U,V)}{\text{d}\bar{u}_i}$ is a row vector, and $\frac{\text{d}f(U,V)}{\text{d}\bar{v}_j}$ is a column vector, of length \emph{nDims}.

In \emph{SAG}, storing only the summed-up gradients is not sufficient for matrix factorization.
The reason is that, each iteration of \emph{SAG} requires the fine-grain gradients of individual entries 
(e.g. $\frac{\text{d}f(\bar{u}_i,\bar{v}_j)}{\text{d}\bar{u}_i}$ and $\frac{\text{d}f(\bar{u}_i,\bar{v}_j)}{\text{d}\bar{v}_j}$) 
that we previously sampled at an iteration before \emph{t}.  
As we will prove, when directly applied to matrix factorization without using our \tool approach, \emph{SAG} will have a asymptotic space complexity of $\theta(nDims*(min(M,N)+nRows+nCols))$.
\emph{M} is the number of \emph{distinct} entries that we have previously sampled.  At any iteration $t$, 
\begin{equation} \label{eq:msampled}
M \propto \sum_{l=1}^{t} B_l
\end{equation}
%\begin{equation} \label{mlessthan}
%M \leq \sum_{l=1}^{t} B_l
%\end{equation}
$B_r$ is the batch size at ieration $l$; $l \leq t$.
Usually the batch size $B$ is constant for all iterations; then $M$ is proportional to and is less than or equal to $B*t$.

Here, we want to point out that \tool preserves the low asymptotic time complexity as \emph{SAG}; and \tool reduces asymptotic space complexity to $\theta(N+nDims*(nRows+nCols))$. 
We will prove that this asymptotic space complexity is as compact as any memory-less approach.


\header{Gradient Methods in Matrix Factorization.}
Gradient methods are iterative methods of optimization.  
When we increase the number of iterations, we expect the quality of optimization to also increase over time.
At each iteration, gradient methods sample a batch of $B$ entries, calculate the gradients of these entries, and use the calculated gradients to update $U$ and $V$ for the next iteration:
\begin{equation} \label{eq:Ut}
U^{t+1} = U^t + \frac{\alpha^{t}}{B}\left(\sum_{b=1}^{B}\frac{\text{d}f(\bar{u}_{entry(b).i}, \bar{v}_{entry(b).j})}{\text{d}\bar{u}_{entry(b).i}}\right)
\end{equation}
\begin{equation} \label{eq:Vt}
V^{t+1} = V^t + \frac{\alpha^{t}}{B}\left(\sum_{b=1}^{B}\frac{\text{d}f(\bar{u}_{entry(b).i}, \bar{v}_{entry(b).j})}{\text{d}\bar{v}_{entry(b).j}}\right)
\end{equation}

At iteration \emph{t}, $U^{t}$ is the current approximation of $U$.
We use the gradients of the sampled batch of entries to update $U^{t}$ into $U^{t+1}$ for iteration \emph{t+1}.

$\alpha^{t}$ is the \emph{learning-rate} or \emph{step-size}, at iteration \emph{t}.  
When the goal of our optimization is to maximize an objective function, we apply \emph{gradient-ascent} on \emph{U} and \emph{V}; thus we set $\alpha^{t} > 0$.
When we try to minimize an objective function, we apply \emph{gradient-descent} and set $\alpha^{t} < 0$.

\emph{entry(b)} is the \emph{b}-th entry in our batch of samples.  
\emph{entry(b).i} is the \emph{row} number of the entry; \emph{entry(b).j} is the \emph{column} number of the entry sampled from $A$.

Full deterministic gradient (\emph{FG}) takes all $N$ samples at each iteration;  $B$ = $N$ in \emph{FG}.
Stochastic gradient (\emph{SG}) takes only one or a few samples per iteration: $B$ is usually a constant much less than $N$.


\header{Stochastic Average Gradient.}
Stochastic Average Gradient (\emph{SAG}) requires a memory of previously-computed gradients: e.g. $\bar{m}_{U}^{t}$ and $\bar{m}_{V}^{t}$ for matrix factorization.
Each iteration of \emph{SAG} uses a sampled batch of entries to update the memory.
After the update, \emph{SAG} then applies the updated memory $\bar{m}_{U}^{t+1}$ and $\bar{m}_{V}^{t+1}$ respectively on calculating $U^{t+1}$ and $V^{t+1}$:
\begin{equation} \label{eq:sag_mi}
  \bar{m}_{entry(b).i}^{t+1} = \frac{\text{d}f(\bar{u}_{entry(b).i}, \bar{v}_{entry(b).j})}{\text{d}\bar{u}_{entry(b).i}}
\end{equation}
\begin{equation} \label{eq:sag_mu}
  \bar{m}_{U}^{t+1} = \bar{m}_{U}^{t} + \sum_{b=1}^{B}\left[\bar{m}_{entry(b).i}^{t+1} - \bar{m}_{entry(b).i}^{t}\right]
\end{equation}
\begin{equation} \label{eq:sag_ut}
  U^{t+1} = U^{t} + \frac{\alpha^{t}}{M}\left(\bar{m}_{U}^{t+1}\right)
\end{equation}
\begin{equation} \label{eq:sag_mj}
  \bar{m}_{entry(b).j}^{t+1} = \frac{\text{d}f(\bar{u}_{entry(b).i}, \bar{v}_{entry(b).j})}{\text{d}\bar{v}_{entry(b).j}}
\end{equation}
\begin{equation} \label{eq:sag_mv}
\bar{m}_{V}^{t+1} = \bar{m}_{V}^{t} + \sum_{b=1}^{B}\left[\bar{m}_{entry(b).j}^{t+1} - \bar{m}_{entry(b).j}^{t}\right]
\end{equation}
\begin{equation} \label{eq:sag_vt}
  V^{t+1} = V^{t} + \frac{\alpha^{t}}{M}\left(\bar{m}_{V}^{t+1}\right)
\end{equation}

$\bar{m}_{entry(b).i}^{t}$ and $\bar{m}_{entry(b).j}^{t}$ are the fine-grain gradients of individual matrix entries that were previously sampled.  

$\bar{m}_{entry(b).i}^{t}$ is a $1$-by-$nRows$ row vector;
$\bar{m}_{entry(b).j}^{t}$ is a $nCols$-by-$1$ column vector.

$\bar{m}_{U}^{t}$ is a $nRows$-by-$nDims$ matrix, because $\bar{m}_{U}^{t}$ aggregates the gradients of all rows in the \emph{user} matrix $U$.
Similarily, $\bar{m}_{V}^{t}$ is a $nDims$-by-$nCols$ matrix.

We apply \emph{SAG} into matrix factorization for two reasons.  
First, \emph{SAG} has iteration cost as low as stochastic gradient (\emph{SG}).
Second, \emph{SAG}'s convergence rate is faster than \emph{SG}, and sometimes as fast as full deterministic gradient (\emph{FG}).

\header{Convergence rate, Iteration cost, and Prototyping recommender systems.}
At a high level, the ideal combination of a fast convergence rate and a low iteration cost implies a better optimization in a shorter amount of time when data-scientists prototype model-based recommender systems.
An intuition behind gradient methods is that, at least for objective functions that are convex, the gradients guide the updates of $U^t$ and $V^t$ towards the direction of optimization.  
Convergence rate measures how many iterations a gradient method is expected to take towards reaching optimization. 
Iteration cost measures how many entries we sample per iteration.

Full deterministic gradient has the best possible convergence rate because each iteration of \emph{FG} samples all $N$ entries in the dataset.
However, while \emph{FG} is guaranteed to take a less number of iterations than \emph{SG} to reach optimization, sampling all $N$ entries per iteration slows down \emph{FG} overall because the optimization process would still take many iterations.
Depending on the mathematical properties of the objective function, stochastic gradient often has much slower convergence rates than \emph{FG} because \emph{SG} samples only one or a few random entries per iteration.
Therefore, while \emph{SG} has the lowest possible $\theta(1)$ iteration cost, overall \emph{SG} is still slow because \emph{SG} would take many more iterations to reach  a quality of optimization similar to \emph{FG}.

\emph{SAG} speeds-up the convergence rate by reusing the gradients of past samples.  
Reusing past gradients enables \emph{SAG} to sample $\theta(1)$ entries per iteration and to achieve the lowest possible iteration cost.  
Our evaluation illustrates that \emph{SAG} gives a better optimization with less time than \emph{FG} and \emph{SG}.  
In this paper, we minimize the drawbacks or costs of using \emph{SAG} in matrix factorization while preserving \emph{SAG}'s benefits.
