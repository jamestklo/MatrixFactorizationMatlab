\section{Evalulation}


\subsection{Research Questions.}

\header{Quality vs. Time}
Our first set of research questions focuses on optimization quality vs. time.
Given more time, any gradient method yields a better optimization.  
Our focus here is to identify which gradient method is the most suitable for data-scientists prototyping recommender systems.
In terms of suitability, we mean the gradient method that yields the best quality optimization within the shortest amount of time.
Here, we consider the general \emph{SAG} approach \emph{as is}. 
The next set of research questions studies the specific \emph{space vs. time} trade-off between \tool and the na$\ddot{i}ve$ approach to \emph{SAG}.

Between \emph{SAG}, full deterministic gradient and stochastic gradient,
\begin{sloppy}
\begin{compactenum}
\item Which gradient method yields a better optimization given the same amount of time?
\item Which gradient method uses the shortest amount of time to reach a similar quality of optimization?
\item Can \emph{SAG} and specifically \tool work well with different objective functions in recommender systems?
\item Can \emph{SAG} and specifically \tool work well with different matrix datasets?
\end{compactenum}
\end{sloppy}


\header{Space vs. Time}
Our second set of research questions investigates whether re-computing is worth the additional time.
Here, we investigate the actual space vs. time trade-off between \tool vs. the na$\ddot{i}$ve approach to \emph{SAG}:
Compared to the na$\ddot{i}ve$ approach to SAG, in practice
\begin{sloppy}
\begin{compactitem}
\item How much slower is \tool due to re-computing?
\item How much memory does \tool save?
\end{compactitem}
\end{sloppy}



\header{Objective Functions.}
% cite objective functions



\header{Datasets}
% cite datasets



\header{Experiment Setup}
% a table?