\section{Introduction}
% Why RecSys
Shopping, text advertising, display advertising, renting movies, listening to musicâ€¦ recommender systems are prevalent and ubiquitous in our daily lives.  
% Why Matrix Factorization  
Matrix factorization (\emph{MF}) is a popular technique in model-based recommender systems.  
\emph{MF} has been utilized extensively in past research for handling both explicit \cite{mmmf2005fast, mnar, gapfm} ratings, and implicit \cite{wrmf2008hu, wrmf2008pan, climf, bpr, mnar} feedback.  

% Pitfalls with FG and SG  
In recommender systems that utilize matrix factorization, most optimize an objective function.  
Full deterministic gradient (\emph{FG}) and stochastic gradient (\emph{SG}) are the two main gradient methods for optimization.  
All of the recommender systems that we cite above utilize either full deterministic gradient, or stochastic gradient.

Unfortunately, both full deterministic gradient and stochastic gradient have pitfalls when it comes to prototyping recommender systems.  
Full deterministic gradient can offer high quality optimizations.  
However, \emph{FG} is slow because in each iteration of optimization, \emph{FG} has to pass through all the samples in the dataset.  
Stochastic gradient is relatively fast; its iteration cost is low because each iteration of \emph{SG} looks at only one or a few samples.  
However, the trade-off with stochastic gradient is that it often provides low quality optimizations.  
By chance, stochastic gradient may \emph{eventually} yield a good quality optimization. 
If it ever happens, it is after a tremendous number of iterations.  
Thus stochastic gradient is also slow in terms of yielding a good quality optimization within a reasonable amount of time.

High quality optimizations within a short amount of time is important when building recommender systems.  
The first reason is that data scientists often have to run repeated experiments: e.g. with different objective functions, different metrics, different datasets, and different optimization parameters.  
The high level goal to run multiple experiments is that, through experimentation and comparing results of multiple trials, 
data scientists can ultimately get a sufficiently good mix of objective function and hyper parameters for fitting a dataset.  
The second reason is that product life cycles are shortening in the age of agile software engineering.  
Thus data scientists are often faced with the challenge of running more experiments and producing high quality results with less time.  
% how does SAG help achieve this goal: make it obvious

%Why SAG in prototyping model-based recommender systems  
In this paper, we study the challenge from the perspective of convex-optimization.  
We propose using the stochastic average gradient (\emph{SAG}) method \cite{schmidt2013minimizing, roux2012stochastic} as a viable alternative to using \emph{FG} and \emph{SG} during the prototyping process.  
\emph{SAG} has the distinctive advantage that its optimization quality is proven to be much better than \emph{SG}; at the same time \emph{SAG}'s iteration cost is asymptotically as low as \emph{SG}.  
However, applying and adapting \emph{SAG} to matrix factorization is not trivial because \emph{SAG} requires previously-computed gradients; 
and storing these gradients can lead to very high asymptotic space complexity.  
We explore the challenge with space-complexity, and resolve it by proposing a re-computation approach (\tool) that re-computes the previously-computed gradients on-the-fly, on-demand.  
\tool preserves the fast convergence rate and low iteration cost of \emph{SAG}.  
Moreover, the asymptotic space complexity of \tool is as compact as memory-less gradient methods such as \emph{FG} and \emph{SG}.  

To the best of our knowledge, we are the first to
\begin{sloppy}
\begin{compactitem}
\item Identify pitfalls associated with using full deterministic gradient and stochastic gradient when data-scientists prototype model-based recommender systems.  
\item Propose Stochastic Average Gradient (\emph{SAG}) as a viable alternative for yielding higher quality optimizations while enjoying a low iteration cost.  
\item Extend \emph{SAG} into \tool for matrix factorization, resolve the space complexity challenge in adapting \emph{SAG} from the domain of large-scale supervised-machine-learning into the domain of prototyping recommender algorithms.  
\item Prove in theory, that \tool has a convergence rate as fast as the original \emph{SAG}; \tool has asymptotic time complexity as efficient as any gradient method with the lowest iteration cost, and \tool has asymptotic space complexity as compact as any memoryless gradient method.  
\item Extensively evaluate and compare \tool across multiple RecSys objective functions and diverse datasets.  
\item Demonstrate in practice that, even without any optimization or fine-tuning on the implementation, \tool still yields the best optimization within the shortest time despite the additional time of re-computation, and that \tool uses memory at a level similar to the memory-less stochastic gradient. 
\item Provide an follow-up experiment and evidence that both full deterministic gradient and stochastic gradient would take much longer for reaching a quality of optimization similar to \tool.
\end {compactitem}
\end{sloppy}
