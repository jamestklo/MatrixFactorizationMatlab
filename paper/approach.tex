\section{approach}
Similar to the chain-rule approach, \tool does not store and re-computes $\bar{m}_{entry(b).i}^{t}$ in equation \ref{eq:sag_mu} and $\bar{m}_{entry(b).j}^{t}$ in equation \ref{eq:sag_mv}:

\begin{equation} \label{eq:sagre_mi}
  \bar{m}_{entry(b).i}^{t} =\:recomputed\:\:\frac{\text{d}f(\bar{u}_{entry(b).i}^{s}, \bar{v}_{entry(b).j}^{s})}{\text{d}\bar{u}_{entry(b).i}^{s}}
\end{equation}

\begin{equation} \label{eq:sagre_mj}
  \bar{m}_{entry(b).j}^{t} =\:recomputed\:\:\frac{\text{d}f(\bar{u}_{entry(b).i}^{s}, \bar{v}_{entry(b).j}^{s})}{\text{d}\bar{u}_{entry(b).j}^{s}}
\end{equation}

The chain-rule approach is undesirable because it must store $min(M,N)$ different copies of past versions of $\bar{m}_{entry(b).i}^{t}$ and $\bar{m}_{entry(b).j}^{t}$.
There are two problems.  First, each entry can come from a different iteration; or different entries can come from different iterations.
Second, the same entry may get sampled more than once at two or more different iterations.

To save space, we must store as few copies of $\bar{m}_{entry(b).i}^{t}$ and $\bar{m}_{entry(b).j}^{t}$ as possible.
\tool resolves the two problems above with two steps.
First, \tool predicts ahead the entires that we are going to sample.
Second, \tool performs a full deterministic gradient \emph{FG} just before \tool re-samples the same entry.

At the iteration that \tool performs a full deterministic gradient, we call it iteration $s$, \tool stores 4 matrices:
\begin{sloppy}
\begin{compactitem}
\item the actual \emph{user} matrix $U$ at iteration $s$: $U^{s}$
\item the actual \emph{item} matrix $V$ at iteration $s$: $V^{s}$
\item aggregated memory gradient for \emph{user} matrix $U$: $\bar{m}_{U}^{s}$
\item aggregated memory gradient for \emph{item} matrix $V$: $\bar{m}_{V}^{s}$
\end{compactitem}
\end{sloppy}

We should distinguish that $U^{s}$ and $V^{s}$ are stored \emph{just before} \tool performs a full deterministic gradient at iteration $s$.
The significance is that we will use $U^{s}$ and $V^{s}$ to re-compute the fine-grain memory gradients at future iterations $t > s$.

$\bar{m}_{U}^{s}$ and $\bar{m}_{V}^{s}$ are the direct outcome results of the full deterministic gradient.  
The reason is that \emph{FG} samples all $N$ entries and thus resets every possible fine-grain gradient in memory.
Thus we store $\bar{m}_{U}^{s}$ and $\bar{m}_{V}^{s}$ \emph{after} \tool performs an iteration of \emph{FG}.

At the iterations $t$ in between \tool performs two \emph{FG}'s, e.g. $s < t < s'$, \tool performs iterations of ordinary \emph{SAG}.
When \tool performs ordinary \emph{SAG}, \tool computes but does \textbf{not store} the latest version of fine-grain gradients of individual entries: 
\[
\bar{m}_{entry(b).i}^{t+1} = \frac{\text{d}f(\bar{u}_{entry(b).i}, \bar{v}_{entry(b).j})}{\text{d}\bar{u}_{entry(b).i}}\:in\:equation\:\ref{eq:sag_mi}
\]
\[
\bar{m}_{entry(b).j}^{t+1} = \frac{\text{d}f(\bar{u}_{entry(b).i}, \bar{v}_{entry(b).j})}{\text{d}\bar{v}_{entry(b).j}}\:in\:equation\:\ref{eq:sag_mj}
\]
\tool simply updates $\bar{m}_{U}^{s}$ and $\bar{m}_{V}^{s}$ with the newly computed fine-grain gradients, as equation \ref{eq:sag_mu} and equation \ref{eq:sag_mv} show.

After we perform an iteration of \emph{FG}, we also predict upcoming entries ahead of time.
Therefore, at future iterations $t > s$ after a \emph{FG}, we ensure that the different entries that we are going to sample are \textbf{distinct} before we perform another iteration of full deterministic gradient.
The significance of having \emph{distinct} entries is that, at future iterations $t > s$, we will not overwrite any fine-grain gradient of individual entries 
(e.g. $\bar{m}_{entry(b).i}^{t}$ in equation \ref{eq:sag_mu} and $\bar{m}_{entry(b).j}^{t}$ in equation \ref{eq:sag_mv}).
Therefore, we can \emph{re-compute} all possible fine-grain gradients of individual entries 
from a single copy of the \emph{user} matrix $U^{s}$ and the \emph{item} matrix $V^{s}$, 
that \tool stored at the same iteration $s$.

Before we perform another iteration of \emph{FG}, we do not need to store any $\bar{m}_{entry(b).i}^{t+1}$ or any $\bar{m}_{entry(b).j}^{t+1}$.
The reason is that before we re-sample the same entry, we perform an iteration of \emph{FG}.
The purpose of an iteration of \emph{FG} is to reset all fine-grain gradients of individual entries at the same iteration $s$, 
so that we will not  the fine-grain gradients rather than storing them.
Not storing the newly-computed fine-grain gradients saves $\theta(min(M,N)*nDims)$ space.


\emph{Theorem 5.}
\tool has convergence rate at least as fast as \emph{SAG}.
\begin{proof}

\end{proof}


\emph{Theorem 6.}
\tool has $\theta(1)$ asymptotic time-complexity and is as efficient as both \emph{SAG} and stochastic gradient.
\begin{proof}
\tool achieves the lowest possible asymptotic iteration cost.
\end{proof}


\emph{Theorem 7.}
\tool has $\theta(N + min(M,N) + nDims*(nRows+nCols))$ asymptotic space-complexity and is as compact as any memory-less gradient method.
\begin{proof}
Indeed, \tool achieves the best possible asymptotic space-complexity (\emph{Theorem 2}).
\end{proof}
