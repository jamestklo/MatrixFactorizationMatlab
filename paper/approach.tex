\section{approach}
Similar to the chain-rule approach, \tool re-computes $\bar{m}_{entry(b).i}^{t}$ in equation \ref{eq:sag_mu} and $\bar{m}_{entry(b).j}^{t}$ in equation \ref{eq:sag_mv}:

\begin{equation} \label{eq:sagre_mu}
  \bar{m}_{entry(b).i}^{t} = recomputed  \frac{\text{d}f(\bar{u}_{entry(b).i}^{s}, \bar{v}_{entry(b).j}^{s})}{\text{d}\bar{u}_{entry(b).i}^{s}}
\end{equation}

\begin{equation} \label{eq:sagre_mu}
  \bar{m}_{entry(b).j}^{t} = recomputed  \frac{\text{d}f(\bar{u}_{entry(b).i}^{s}, \bar{v}_{entry(b).j}^{s})}{\text{d}\bar{u}_{entry(b).j}^{s}}
\end{equation}

The chain-rule approach is undesirable because it must store $min(M,N)$ different copies of past versions of $\bar{m}_{entry(b).i}^{t}$ and $\bar{m}_{entry(b).j}^{t}$.
There are two reasons.  First, each entry can come from a different iteration.  Second, the same entry may get sampled at more than one different iterations.

To save space, we must store as few copies of $\bar{m}_{entry(b).i}^{t}$ and $\bar{m}_{entry(b).j}^{t}$ as possible.  
\tool resolves this issue, by predicting ahead the entires that we are going to sample, and ensures that the samples we are going to sample are distinct.  
\tool ensures that 


\emph{Theorem 5.}
\tool has convergence rate at least as fast as \emph{SAG}.
\begin{proof}

\end{proof}


\emph{Theorem 6.}
\tool has $\theta(1)$ asymptotic time-complexity and is as efficient as both \emph{SAG} and stochastic gradient.
\begin{proof}
\tool achieves the lowest possible asymptotic iteration cost.
\end{proof}


\emph{Theorem 7.}
\tool has $\theta(N + min(M,N) + nDims*(nRows+nCols))$ asymptotic space-complexity and is as compact as any memory-less gradient method.
\begin{proof}
Indeed, \tool achieves the best possible asymptotic space-complexity (\emph{Theorem 2}).
\end{proof}
